{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Loan Approval Prediction Model: Senior Data Analyst Assessment for Concept Group.\n",
    "---\n",
    "By Abdulbasit Bello\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Problem\n",
    "We need to predict the outcome of a loan application (approval or denial) using the provided historical data. \n",
    "This is a binary classification problem\n",
    "1. The Input: Features like the credit score, employment lenght, Annual income, loan amount etc.\n",
    "2. The Output: A loan approval decision, approved (1) or denied (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning the Solution\n",
    "Here's a breakdown of the steps I'll take to complete this loan approval prediction model\n",
    "\n",
    "1. **Import Libraries**: Import all necessary Python libraries (pandas, numpy, scikit-learn, matplotlib, seaborn).\n",
    "\n",
    "2. **Load Data**: Load the loan_application_data.csv file into a pandas DataFrame.\n",
    "\n",
    "3. **Exploratory Data Analysis (EDA)**: Perform EDA to understand the data, including data types, summary statistics, distributions of key variables, missing values, and correlations between features. Visualizations are crucial here (histograms, box plots, scatter plots, etc.).\n",
    "\n",
    "4. **Data Cleaning**: Handle missing values (imputation or removal) and outliers (clipping, transformation, or removal). These choices would be justified in the code if missing data/outliers are found.\n",
    "\n",
    "5. **Feature Engineering**: Create new features (Debt-To-Income, Loan-to-Income Ratio, Employment Length,Loan-to-Debt (LTD) Ratio) from existing columns. Handle potential errors (e.g., division by zero, invalid dates).\n",
    "\n",
    "6. **Data Preprocessing**: Prepare the data for modeling. This includes:\n",
    "\n",
    "    1. Encoding categorical features to enable the model \"count\" (one-hot encoding, label encoding).\n",
    "    2. log transformation of numerical features to squish the range between small and large values\n",
    "    3. Scaling numerical features to establish proportionality (standardization, min-max scaling).\n",
    "    4. Splitting the data into training and testing sets.\n",
    "\n",
    "7. **Model Selection and Training**: Choose an appropriate machine learning algorithm (logistic regression, decision tree, random forest, etc.). Train the model on the training data. Use cross-validation for model evaluation.\n",
    "\n",
    "8. **Model Evaluation**: Evaluate the performance of the model on the testing set using relevant metrics (accuracy, precision, recall, F1-score, AUC-ROC). Justification for my choice of metrics in the code for model evaluation. Present results in a clear and concise manner using classification reports and confusion matrices.\n",
    "\n",
    "9. **Model Selection**: Choose the best-performing model based on the evaluation metrics.\n",
    "\n",
    "10. **Model Interpretation**: Interpret the chosen model. For example, analyze feature importances (for tree-based models) or coefficients (for logistic regression) to understand which features are most influential in predicting loan approval.\n",
    "\n",
    "11. **Report Generation**: Create a well-documented report summarizing my findings, including EDA results, data cleaning and feature engineering steps, model selection and evaluation, and model interpretation. Including relevant visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing  import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "print(\"Libraries successfully imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('loan_application_data.csv')\n",
    "    print(\"Data loaded successfully!\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Named file is missing from the directory, kindly double-check the file name and location\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Something's wrong, {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exploratory Data Analysis Part 1 - Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Display first 5 rows of the data\n",
    "print(\"Snippet of the data\\n\")\n",
    "display(df.head())\n",
    "\n",
    "# Display information about the columns of the data and its shape\n",
    "print(\"\\nData Shape:\\n\",df.shape)\n",
    "print(\"\\nDataset Info\")\n",
    "print(df.info())\n",
    "\n",
    "# Display summary statistics about the data\n",
    "print(\"\\nDataset Description\\n\",df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Exploratory Data Analysis Part 1 - Raw Data\n",
    "\n",
    "1. **Dataset Overview**:\n",
    "    - The dataset contains **1000 rows** and **13 columns**.\n",
    "    - Columns include numerical, categorical, and datetime data types.\n",
    "\n",
    "2. **Key Observations**:\n",
    "    - No missing values are present in the dataset.\n",
    "    - Numerical features like `CreditScore`, `AnnualIncome`, and `LoanAmount` have varying ranges.\n",
    "    - Categorical features include `EmploymentStatus` and `LoanOutcome`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for missing data and inconsistencies in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Grouping the features for easier analysis\n",
    "numerical_features = df.select_dtypes(include=['number'])\n",
    "categorical_features = df.select_dtypes(include=['object'])\n",
    "\n",
    "# Create a new figure for LoanOutcome Distribution and EmploymentStatus vs LoanOutcome \n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# LoanOutcome Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='LoanOutcome', data=df)\n",
    "plt.title('Loan Outcome Distribution')\n",
    "plt.xlabel('Loan Outcome')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# EmploymentStatus vs LoanOutcome\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='EmploymentStatus', hue='LoanOutcome', data=df, palette='Set1')\n",
    "plt.title('Employment Status vs Loan Outcome')\n",
    "plt.xlabel('Employment Status')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Distributions of numerical features\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "for i, col in enumerate(numerical_features):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    sns.histplot(df[col], bins=20, kde=True, color='lightcoral')\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection using Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Outlier detection using boxplots\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Generate the boxplots for each numerical feature in the dataset\n",
    "for i, col in enumerate(numerical_features):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    sns.boxplot(x=df['LoanOutcome'], y=df[col])\n",
    "    plt.title(f\"{col} by Loan Outcome\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis using a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Checking for Correlation between numerical columns\n",
    "\n",
    "correlation_matrix = numerical_features.corr()\n",
    "print(\"Correlation Matrix \\n\",correlation_matrix)\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Analyze missing data patterns\n",
    "missing_data_summary = df.isnull().sum().to_frame(name='Missing Count')\n",
    "missing_data_summary['Missing Percentage'] = (missing_data_summary['Missing Count'] / len(df)) * 100\n",
    "missing_data_summary['Loan Outcome'] = df.groupby(df.isnull().any(axis=1))['LoanOutcome'].value_counts()\n",
    "display(missing_data_summary)\n",
    "\n",
    "# Visualize missing data by Loan Outcome (if any missing values exist)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='coolwarm')\n",
    "plt.title('Missing Data Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inclusion of 'Loan Outcome' in the summary helps identify potential biases in missingness. The heatmap visually represents the distribution of missing values, revealing potential patterns.\n",
    "\n",
    "---\n",
    "The plot is empty because there are no missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Unique values in the dataset\n",
    "print(\"\\nUnique values in the dataset/column:\\n\",df.nunique())\n",
    "\n",
    "#Check for inconsistencies (example: unexpected values in categorical columns)\n",
    "print(\"\\nUnique values in EmploymentStatus:\\n\", df['EmploymentStatus'].unique())\n",
    "print(\"\\nUnique values in LoanOutcome:\\n\", df['LoanOutcome'].unique())\n",
    "\n",
    "#Handle inconsistencies (example: standardize capitalization)\n",
    "df['EmploymentStatus'] = df['EmploymentStatus'].str.capitalize() \n",
    "df['LoanOutcome'] = df['LoanOutcome'].str.capitalize()\n",
    "\n",
    "# Handle date columns in the dataset\n",
    "df['EmploymentStartDate'] = pd.to_datetime(df['EmploymentStartDate'], errors='coerce')\n",
    "df['ApplicationDate'] = pd.to_datetime(df['ApplicationDate'], errors='coerce')\n",
    "print(\"\\nDates successfully updated!\\n\",display(df.head()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Debt-to-Income Ratio (DTI)\n",
    "df['DTI'] = df['OutstandingDebt'] / (df['AnnualIncome'] / 12)  # Monthly debt / monthly income\n",
    "\n",
    "# Loan-to-Income Ratio\n",
    "df['LoanToIncome'] = df['LoanAmount'] / df['AnnualIncome']\n",
    "\n",
    "# Credit score bins\n",
    "df['CreditScoreBins'] = pd.cut(df['CreditScore'], bins=[0, 580, 670, 740, 800, 850],labels=['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'])\n",
    "\n",
    "# Employment Length Calculation\n",
    "df['EmploymentLength'] = ((df['ApplicationDate'] - df['EmploymentStartDate']).dt.days / 365.25).fillna(0)\n",
    "\n",
    "# Loan-to-Debt Ratio (Oustanding) (LDR)\n",
    "df['LDR'] = df['LoanAmount'] / df['OutstandingDebt']\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(df.head())\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Additional Feature Engineering\n",
    "df['LoanAmount_log'] = np.log1p(df['LoanAmount']) #Log transformation of LoanAmount\n",
    "df['AnnualIncome_log'] = np.log1p(df['AnnualIncome']) #Log transformation of AnnualIncome\n",
    "df['CreditScore_scaled'] = (df['CreditScore'] - df['CreditScore'].min()) / (df['CreditScore'].max() - df['CreditScore'].min()) #Scale CreditScore\n",
    "display(df)\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.5: Exploratory Data Analysis Part 2 -Enriched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualizing the new features\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "numerical_features2 = ['DTI', 'LoanToIncome', 'EmploymentLength', 'LDR', 'AnnualIncome_log', 'LoanAmount_log', 'CreditScore_scaled']\n",
    "\n",
    "# Plotting the distributions of the new numerical features\n",
    "for i, col in enumerate(numerical_features2):\n",
    "    plt.subplot(5, 2, i + 1)\n",
    "    sns.histplot(df[col], bins=20, kde=True, color='lightcoral')\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "# Plotting the categorical feature\n",
    "plt.subplot(5, 2, i + 2)\n",
    "sns.countplot(x='CreditScoreBins', data=df, palette='Set1')\n",
    "plt.title('Credit Score Bins Distribution')\n",
    "plt.xlabel('Credit Score Bins')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Checking for outliers in the new features using boxplots\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Generate the boxplots for each numerical feature in the dataset\n",
    "for i, col in enumerate(numerical_features2):\n",
    "    plt.subplot(4, 2, i + 1)\n",
    "    sns.boxplot(x=df['LoanOutcome'], y=df[col])\n",
    "    plt.title(f\"{col} by Loan Outcome\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Checking for Correlation between the all numerical columns including the new features\n",
    "numerical_features = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Drop existing columns that have been log transformed or scaled\n",
    "numerical_features = numerical_features.drop(columns=['LoanAmount', 'AnnualIncome', 'CreditScore'])\n",
    " \n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numerical_features.corr()\n",
    "print(\"Correlation Matrix\")\n",
    "display(correlation_matrix)\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Let's write our analysis based on the correlation matrix\n",
    "\n",
    "# Strong Negative Correlations\n",
    "print(\"Strong Negative Correlations:\")\n",
    "print(\"- Annual Income (log) and DTI:\", \"r = -0.605\")\n",
    "print(\"- Annual Income (log) and LoanToIncome:\", \"r = -0.591\")\n",
    "\n",
    "# Moderate to Strong Positive Correlations\n",
    "print(\"\\nModerate to Strong Positive Correlations:\")\n",
    "print(\"- Outstanding Debt and DTI:\", \"r = 0.692\")\n",
    "print(\"- Loan Amount (log) and LoanToIncome:\", \"r = 0.630\")\n",
    "\n",
    "# Key Insights\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"1. Higher annual income is associated with lower debt-to-income ratios\")\n",
    "print(\"2. Higher annual income corresponds to lower loan-to-income ratios\")\n",
    "print(\"3. Outstanding debt strongly influences debt-to-income ratio\")\n",
    "print(\"4. Loan amount has a strong positive relationship with loan-to-income ratio\")\n",
    "print(\"5. Credit score shows very weak correlations with other features (all r < 0.05)\")\n",
    "print(\"6. Employment length shows minimal correlation with other variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 1. One-hot encoding ONLY (remove label encoding)\n",
    "df = pd.get_dummies(df, columns=['EmploymentStatus', 'CreditScoreBins'], drop_first=True)\n",
    "numerical_features = df.select_dtypes(include=['number'])\n",
    "display(numerical_features.head())\n",
    "\n",
    "# 2. Keep target simple (no encoding needed if already binary)\n",
    "# Ensure LoanOutcome is binary (0/1)\n",
    "df['LoanOutcome'] = np.where(df['LoanOutcome'] == 'Approved', 1, 0)\n",
    "df['LoanOutcome'] = df['LoanOutcome'].astype(int)\n",
    "\n",
    "# 3. Select ALL relevant features (including one-hot encoded ones)\n",
    "display(df.head())\n",
    "features = ['CreditScore', 'AnnualIncome', 'LoanAmount', 'OutstandingDebt',\n",
    "           'EmploymentLength', 'DTI', 'LoanToIncome'] + \\\n",
    "           [col for col in df.columns if 'EmploymentStatus_' in col or 'CreditScoreBins_' in col]\n",
    "\n",
    "# 4. Split before scaling to avoid data leakage\n",
    "X = df[numerical_features.drop(columns=['LoanOutcome','ApplicationDate', 'EmploymentStartDate'])]\n",
    "y = df['LoanOutcome']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Scale only numerical features (not the one-hot encoded ones)\n",
    "numerical_features = ['CreditScore', 'AnnualIncome', 'LoanAmount', \n",
    "                     'OutstandingDebt', 'EmploymentLength', 'DTI', 'LoanToIncome']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test[numerical_features] = scaler.transform(X_test[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#One-hot encoding for categorical features\n",
    "df = pd.get_dummies(df, columns=['EmploymentStatus', 'LoanOutcome', 'CreditScoreBins'], drop_first=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "df['EmploymentStatus'] = le.fit_transform(df['EmploymentStatus'])\n",
    "df['LoanOutcome'] = le.fit_transform(df['LoanOutcome'])  # 1 for Approved, 0 for Denied\n",
    "\n",
    "# Select features and target\n",
    "features = ['CreditScore', 'AnnualIncome', 'LoanAmount', 'OutstandingDebt', \n",
    "           'EmploymentStatus', 'EmploymentLength', 'DTI', 'LoanToIncome']\n",
    "X = df[features]\n",
    "y = df['LoanOutcome']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "The loan-to-debt ratio (LDR) might not have been selected as a feature for the model due to one or more of the following reasons:\n",
    "\n",
    "1. **Low Correlation with Target Variable**: If the correlation analysis shows that LDR has little to no relationship with the target variable (`LoanOutcome`), it may not contribute significantly to the model's predictive power.\n",
    "\n",
    "2. **Multicollinearity**: LDR might be highly correlated with other features like `LoanAmount` or `OutstandingDebt`. Including highly correlated features can lead to multicollinearity, which can negatively impact the model's performance and interpretability.\n",
    "\n",
    "3. **Outliers**: The LDR feature might contain extreme outliers (as observed in earlier boxplots), which could skew the model's predictions. If these outliers are not addressed, the feature might be excluded.\n",
    "\n",
    "4. **Feature Importance**: During feature selection, techniques like recursive feature elimination (RFE) or tree-based feature importance might have indicated that LDR is not a significant predictor.\n",
    "\n",
    "5. **Domain Knowledge**: Based on domain expertise, LDR might not be considered as critical as other features like `CreditScore`, `DTI`, or `LoanToIncome` in determining loan approval outcomes.\n",
    "\n",
    "If you believe LDR could be important, you can include it in the model and evaluate its impact on performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
